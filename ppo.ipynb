{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd08420e9e93c695d97be91f5272eab9214f2b8a154faa67cd6133f765f4ca85a58",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "8420e9e93c695d97be91f5272eab9214f2b8a154faa67cd6133f765f4ca85a58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\documents francis\\universit√©\\session8\\inf8225\\projet\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from scipy.stats import multivariate_normal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import MultivariateNormal, Beta\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "env = gym.make('CarRacing-v0')\n",
    "env = Monitor(env, './video', video_callable=lambda episode_id: episode_id%100==0, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, mean_actions, std_actions, old_actions):\n",
    "        # cov_mat = torch.diag(std_actions.mean(dim=0))\n",
    "        distribution = Beta(mean_actions, std_actions)\n",
    "        actions_with_exploration = distribution.sample()\n",
    "\n",
    "        if old_actions is None:\n",
    "            log_actions = distribution.log_prob(actions_with_exploration)\n",
    "        else:\n",
    "            log_actions = distribution.log_prob(old_actions)\n",
    "\n",
    "        return actions_with_exploration, log_actions, distribution.entropy()\n",
    "\n",
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(CNN_Network, self).__init__()\n",
    "\n",
    "        self.conv2d_0 = nn.Conv2d(1, 8, kernel_size=4, stride=2)\n",
    "        self.relu_0 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(8, 16, kernel_size=3, stride=2)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_3 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.relu_3 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu_4 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_5 = nn.Conv2d(128, 256, kernel_size=3, stride=2)\n",
    "        self.relu_5 = nn.ReLU()\n",
    "\n",
    "        # self.action_linear_1 = nn.Linear(4608, 256)\n",
    "        self.action_mean = nn.Linear(256, output_size)\n",
    "        self.action_std = nn.Linear(256, output_size)\n",
    "        self.mean_activation = nn.Softplus()\n",
    "        self.std_activation = nn.Softplus()\n",
    "        \n",
    "        self.gaussian = Gaussian()\n",
    "\n",
    "\n",
    "        # self.value_linear_1 = nn.Linear(4608, 256)\n",
    "        # self.relu_value_1 = nn.LeakyReLU()\n",
    "        self.value_linear_2 = nn.Linear(256, 1)\n",
    "\n",
    "        for layer in [self.conv2d_0 , self.conv2d_1, self.conv2d_2, self.conv2d_3, self.conv2d_4, self.conv2d_5, self.action_mean, self.action_std,\\\n",
    "                      self.value_linear_2]:\n",
    "            torch.nn.init.xavier_normal_(layer.weight)\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, old_actions=None):\n",
    "        x = self.relu_0(self.conv2d_0(x))\n",
    "        x = self.relu_1(self.conv2d_1(x))\n",
    "        x = self.relu_2(self.conv2d_2(x))\n",
    "        x = self.relu_3(self.conv2d_3(x))\n",
    "        x = self.relu_4(self.conv2d_4(x))\n",
    "        x = self.relu_5(self.conv2d_5(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x_action_mean = self.mean_activation(self.action_mean(x))\n",
    "        x_action_std = self.std_activation(self.action_std(x))\n",
    "        actions, log_actions, entropy = self.gaussian(x_action_mean, x_action_std, old_actions)\n",
    "        # x_action = self.activation_action1(x_action)\n",
    "\n",
    "        # x_value = self.relu_value_1(self.value_linear_1(x))\n",
    "        x_value = self.value_linear_2(x)\n",
    "        return actions, log_actions, entropy, x_value\n",
    "\n",
    "agent = CNN_Network(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(list(agent.children())[-2].parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class OrnsteinUhlenbeckActionNoise():\n",
    "#     def __init__(self, mu, sigma=0.3, theta=.10, dt=1e-2, x0=None):\n",
    "#         self.theta = theta\n",
    "#         self.mu = mu\n",
    "#         self.sigma = sigma\n",
    "#         self.dt = dt\n",
    "#         self.x0 = x0\n",
    "#         self.reset()\n",
    "\n",
    "#     def __call__(self):\n",
    "#         x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "#                 self.sigma * self.dt**(1/2) * torch.normal(mean=0.0, std=1.0, size=self.mu.shape, device=device)\n",
    "#         self.x_prev = x\n",
    "#         return x\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.x_prev = self.x0 if self.x0 is not None else torch.zeros_like(self.mu, device=device)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(tensor):\n",
    "    return torch.tensor(tensor.copy(), dtype=torch.float32, device=device)\n",
    "\n",
    "def get_action(state):\n",
    "    actions, log_actions, _, _ = agent(state)\n",
    "    actions = actions[0]\n",
    "    log_actions = log_actions[0]\n",
    "\n",
    "    return  actions.cpu().numpy(), log_actions.cpu().numpy()\n",
    "\n",
    "GAMMA = 0.99\n",
    "def rollout(render, exploration_scale):\n",
    "\n",
    "    state = env.reset() / 255.\n",
    "    memory = []\n",
    "    timesteps = 0\n",
    "    done = False\n",
    "    streak = 0\n",
    "    while not done:\n",
    "        \n",
    "        action, log_action = get_action(to_torch(state).mean(dim=2).reshape(1, 1, state.shape[0], state.shape[1]))\n",
    "        \n",
    "        fixed_action = action.copy()*[2.,1.3,1.5] - [1.,0.3,0.5]\n",
    "        next_state, reward, done, _ = env.step(fixed_action)\n",
    "\n",
    "        # reward += 0.1*np.clip(action[1], 0, 1)\n",
    "        next_state = next_state / 255.\n",
    "\n",
    "        memory.append([state, action, reward, log_action])\n",
    "        timesteps += 1\n",
    "        state = next_state\n",
    "            \n",
    "\n",
    "    states, actions, rewards, log_actions = map(np.array, zip(*memory))\n",
    "\n",
    "    discounted_rewards = np.zeros((len(rewards)))\n",
    "    discount = 0\n",
    "    # Discounts rewards in reverse\n",
    "    for i in reversed(range(len(rewards))):\n",
    "\n",
    "        # Discount fowards from the future for previous states\n",
    "        discount = rewards[i] + discount*GAMMA \n",
    "        discounted_rewards[i] = discount\n",
    "\n",
    "    return to_torch(states).mean(dim=3).unsqueeze(dim=1), to_torch(actions), to_torch(discounted_rewards).reshape(-1,1), to_torch(log_actions), timesteps, np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_EPSILON = 0.2\n",
    "value_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "def get_log_probs_and_value(states, old_actions):\n",
    "    actions, log_actions_new_policy, entropy, values = agent(states, old_actions)\n",
    "\n",
    "    return actions, log_actions_new_policy, values, entropy\n",
    "\n",
    "def compute_advantages(states, rewards):\n",
    "    _, _, _, values = agent(states)\n",
    "\n",
    "    advantages = rewards - values.detach()\n",
    "    advantages = (advantages - advantages.mean()) / \\\n",
    "                (advantages.std() + 1e-8) \n",
    "\n",
    "    return advantages\n",
    "\n",
    "def compute_losses(states, actions, rewards, log_actions, advantages):\n",
    "    # Compute policy loss first\n",
    "    # Compute ratios \n",
    "    new_actions, log_actions_new_policy, values, entropy = get_log_probs_and_value(states, actions)\n",
    "    ratios = torch.exp(log_actions_new_policy - log_actions) \n",
    "\n",
    "    policy_loss = torch.min(ratios*advantages, torch.clip(ratios, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON)*advantages)\n",
    "    policy_loss = -torch.mean(policy_loss)\n",
    "\n",
    "    # Compute value loss\n",
    "    value_loss = value_loss_fn(rewards, values)\n",
    "\n",
    "    # Compute entropy loss\n",
    "    entropy_loss = -torch.mean(entropy)\n",
    "    \n",
    "    return policy_loss, value_loss, entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOS\n",
    "# 1. new_actions / actions - > en log_probs (log_new_actions - log_actions)\n",
    "# 2. Trouver des meilleurs mani√®res pour explorer\n",
    "# 3. tweak value_factor, GAMMA, exploration_factor, lr \n",
    "# 4. Tweak le mod√®le pytorch, activations\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    n_time_steps = 2000000\n",
    "    n_updates_per_episode = 5\n",
    "    \n",
    "    value_factor = 0.2\n",
    "    entropy_factor = 0.01\n",
    "\n",
    "    current_time_step = 0\n",
    "    exploration_factor = 0.995\n",
    "\n",
    "    agent_optimizer = optim.Adam(agent.parameters(), lr=0.0001)\n",
    "    scores = []\n",
    "    entropies = []\n",
    "    while current_time_step < n_time_steps:\n",
    "        # try:\n",
    "        with torch.no_grad():\n",
    "            agent.eval()\n",
    "            states, actions, rewards, log_actions, timesteps, episode_score = rollout(len(scores)%5==0, exploration_factor**len(scores))\n",
    "        # except:\n",
    "        #     continue\n",
    "        scores.append(episode_score)\n",
    "        print(f\"Current score: {scores[-1]}\")\n",
    "        print(f\"Total timesteps: {current_time_step}\")\n",
    "        current_time_step += timesteps\n",
    "        advantages = compute_advantages(states, rewards)\n",
    "\n",
    "        agent.train()\n",
    "        for _ in range(n_updates_per_episode):\n",
    "            agent_optimizer.zero_grad()\n",
    "            policy_loss, value_loss, entropy_loss = compute_losses(states, actions, rewards, log_actions, advantages)\n",
    "\n",
    "            loss = policy_loss + value_factor*value_loss + entropy_factor*entropy_loss\n",
    "            print(f'loss: {loss}')\n",
    "            loss.backward()\n",
    "\n",
    "            agent_optimizer.step()\n",
    "            agent_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    \n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump({\"scores\": scores}, f)\n",
    "        \n",
    "    torch.save(agent, 'agent.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 1188..1489 -> 301-tiles track\n",
      "Current score: -43.23333333333396\n",
      "Total timesteps: 0\n",
      "loss: 0.8160398602485657\n",
      "loss: 0.8143383860588074\n",
      "loss: 0.8126128911972046\n",
      "loss: 0.8108115196228027\n",
      "loss: 0.8089419007301331\n",
      "Track generation: 1103..1390 -> 287-tiles track\n",
      "Current score: -40.459440559441184\n",
      "Total timesteps: 999\n",
      "loss: 0.7435081601142883\n",
      "loss: 0.7413901686668396\n",
      "loss: 0.7391061186790466\n",
      "loss: 0.7366225123405457\n",
      "loss: 0.7339299917221069\n",
      "Track generation: 1167..1463 -> 296-tiles track\n",
      "Current score: -42.27288135593284\n",
      "Total timesteps: 1998\n",
      "loss: 0.794545590877533\n",
      "loss: 0.7915090322494507\n",
      "loss: 0.788148045539856\n",
      "loss: 0.7844306826591492\n",
      "loss: 0.7803208827972412\n",
      "Track generation: 1101..1380 -> 279-tiles track\n",
      "Current score: -38.74892086330983\n",
      "Total timesteps: 2997\n",
      "loss: 0.6916165947914124\n",
      "loss: 0.686255156993866\n",
      "loss: 0.6803322434425354\n",
      "loss: 0.6737878322601318\n",
      "loss: 0.6665197014808655\n",
      "Track generation: 1239..1553 -> 314-tiles track\n",
      "Current score: -42.39201277955332\n",
      "Total timesteps: 3996\n",
      "loss: 0.7279029488563538\n",
      "loss: 0.7190073132514954\n",
      "loss: 0.7091392874717712\n",
      "loss: 0.6981858015060425\n",
      "loss: 0.6860281825065613\n",
      "Track generation: 1043..1308 -> 265-tiles track\n",
      "Current score: -39.29393939394001\n",
      "Total timesteps: 4995\n",
      "loss: 0.6499410271644592\n",
      "loss: 0.6360651850700378\n",
      "loss: 0.6207343935966492\n",
      "loss: 0.6038230061531067\n",
      "loss: 0.5852212905883789\n",
      "Track generation: 1262..1581 -> 319-tiles track\n",
      "Current score: -46.44088050314534\n",
      "Total timesteps: 5994\n",
      "loss: 0.6565781831741333\n",
      "loss: 0.6322358250617981\n",
      "loss: 0.6054059863090515\n",
      "loss: 0.5757914185523987\n",
      "loss: 0.5429931282997131\n",
      "Track generation: 1106..1387 -> 281-tiles track\n",
      "Current score: -39.18571428571497\n",
      "Total timesteps: 6993\n",
      "loss: 0.3570994436740875\n",
      "loss: 0.3237592279911041\n",
      "loss: 0.2893883287906647\n",
      "loss: 0.2550978660583496\n",
      "loss: 0.22257238626480103\n",
      "Track generation: 1127..1413 -> 286-tiles track\n",
      "Current score: -39.885964912280926\n",
      "Total timesteps: 7992\n",
      "loss: 0.39231938123703003\n",
      "loss: 0.3595939576625824\n",
      "loss: 0.3317306637763977\n",
      "loss: 0.31138715147972107\n",
      "loss: 0.3007839024066925\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Tried to reset environment which is not done. While the monitor is active for CarRacing-v0, you cannot call reset() unless the episode is over.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-8fa9954b49e1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexploration_factor\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#     continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-e565078b7a87>\u001b[0m in \u001b[0;36mrollout\u001b[1;34m(render, exploration_scale)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexploration_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtimesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\documents francis\\universit√©\\session8\\inf8225\\projet\\gym\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\documents francis\\universit√©\\session8\\inf8225\\projet\\gym\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\documents francis\\universit√©\\session8\\inf8225\\projet\\gym\\gym\\wrappers\\monitoring\\stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for CarRacing-v0, you cannot call reset() unless the episode is over."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd08420e9e93c695d97be91f5272eab9214f2b8a154faa67cd6133f765f4ca85a58",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "8420e9e93c695d97be91f5272eab9214f2b8a154faa67cd6133f765f4ca85a58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from scipy.stats import multivariate_normal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(CNN_Network, self).__init__()\n",
    "\n",
    "        self.conv2d_0 = nn.Conv2d(1, 16, kernel_size=4, stride=2)\n",
    "        self.relu_0 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.relu_1 = nn.LeakyReLU()\n",
    "\n",
    "        # self.conv2d_2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        # self.relu_2 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2d_3 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.relu_3 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2d_4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu_4 = nn.LeakyReLU()\n",
    "\n",
    "        # self.conv2d_5 = nn.Conv2d(128, 256, kernel_size=3, stride=2)\n",
    "        # self.relu_5 = nn.LeakyReLU()\n",
    "\n",
    "        self.action_linear_1 = nn.Linear(4608, 256)\n",
    "        self.relu_action_1 = nn.LeakyReLU()\n",
    "        self.action_linear_2 = nn.Linear(256, output_size)\n",
    "        self.activation_action1 = nn.Tanh()\n",
    "        self.activation_action2 = nn.ReLU()\n",
    "\n",
    "        self.value_linear_1 = nn.Linear(4608, 256)\n",
    "        self.relu_value_1 = nn.LeakyReLU()\n",
    "        self.value_linear_2 = nn.Linear(256, 1)\n",
    "\n",
    "        for layer in [self.conv2d_0 , self.conv2d_1,self.conv2d_3, self.conv2d_4, self.action_linear_1, self.action_linear_2, self.value_linear_1, self.value_linear_2]:\n",
    "            torch.nn.init.xavier_normal_(layer.weight)\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu_0(self.conv2d_0(x))\n",
    "        x = self.relu_1(self.conv2d_1(x))\n",
    "        # x = self.relu_2(self.conv2d_2(x))\n",
    "        x = self.relu_3(self.conv2d_3(x))\n",
    "        x = self.relu_4(self.conv2d_4(x))\n",
    "        # x = self.relu_5(self.conv2d_5(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x_action = self.relu_action_1(self.action_linear_1(x))\n",
    "        x_action = self.action_linear_2(x_action)\n",
    "        # x_action = self.activation_action1(x_action)\n",
    "\n",
    "        x_value = self.relu_value_1(self.value_linear_1(x))\n",
    "        x_value = self.value_linear_2(x_value)\n",
    "        return x_action, x_value\n",
    "\n",
    "agent = CNN_Network(env.action_space.shape[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CNN_Network(\n  (conv2d_0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2))\n  (relu_0): LeakyReLU(negative_slope=0.01)\n  (conv2d_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n  (relu_1): LeakyReLU(negative_slope=0.01)\n  (conv2d_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n  (relu_3): LeakyReLU(negative_slope=0.01)\n  (conv2d_4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (relu_4): LeakyReLU(negative_slope=0.01)\n  (action_linear_1): Linear(in_features=4608, out_features=256, bias=True)\n  (relu_action_1): LeakyReLU(negative_slope=0.01)\n  (action_linear_2): Linear(in_features=256, out_features=3, bias=True)\n  (activation_action1): Tanh()\n  (activation_action2): ReLU()\n  (value_linear_1): Linear(in_features=4608, out_features=256, bias=True)\n  (relu_value_1): LeakyReLU(negative_slope=0.01)\n  (value_linear_2): Linear(in_features=256, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class OrnsteinUhlenbeckActionNoise():\n",
    "#     def __init__(self, mu, sigma=0.3, theta=.10, dt=1e-2, x0=None):\n",
    "#         self.theta = theta\n",
    "#         self.mu = mu\n",
    "#         self.sigma = sigma\n",
    "#         self.dt = dt\n",
    "#         self.x0 = x0\n",
    "#         self.reset()\n",
    "\n",
    "#     def __call__(self):\n",
    "#         x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "#                 self.sigma * self.dt**(1/2) * torch.normal(mean=0.0, std=1.0, size=self.mu.shape, device=device)\n",
    "#         self.x_prev = x\n",
    "#         return x\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.x_prev = self.x0 if self.x0 is not None else torch.zeros_like(self.mu, device=device)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(tensor):\n",
    "    return torch.tensor(tensor.copy(), dtype=torch.float32, device=device)\n",
    "\n",
    "cov_exploration_matrix = torch.tensor([0.1, 0.06, 0.06], device=device)\n",
    "cov_mat = torch.diag(cov_exploration_matrix)\n",
    "def get_action(state):\n",
    "    actions, values = agent(state)\n",
    "    actions = actions[0]\n",
    "    distribution = MultivariateNormal(actions, cov_mat)\n",
    "    actions_with_exploration = distribution.sample()\n",
    "    log_actions = distribution.log_prob(actions_with_exploration)\n",
    "\n",
    "    return actions_with_exploration.cpu().numpy(), log_actions.cpu().numpy()\n",
    "\n",
    "GAMMA = 0.97\n",
    "def rollout(render):\n",
    "\n",
    "    state = env.reset() / 255.\n",
    "    memory = []\n",
    "    timesteps = 0\n",
    "    done = False\n",
    "    streak = 0\n",
    "    while not done:\n",
    "        \n",
    "        action, log_action = get_action(to_torch(state).mean(dim=2).reshape(1, 1, state.shape[0], state.shape[1]))\n",
    "\n",
    "        for i in range(4):\n",
    "            if render or timesteps%100==0:\n",
    "                env.render()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            \n",
    "\n",
    "            if np.mean(next_state[:, :, 1]) > 185.0:\n",
    "                reward -= 0.05\n",
    "\n",
    "            if reward < 0:\n",
    "                streak +=1\n",
    "                if streak > 250.:\n",
    "                    reward -= 1\n",
    "                    done = True\n",
    "            else:\n",
    "                streak = 0\n",
    "\n",
    "            next_state = next_state / 255.\n",
    "            memory.append([state, action, reward, log_action])\n",
    "            timesteps += 1\n",
    "            state = next_state\n",
    "\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "\n",
    "    states, actions, rewards, log_actions = map(np.array, zip(*memory))\n",
    "\n",
    "    discounted_rewards = np.zeros((len(rewards)))\n",
    "    discount = 0\n",
    "    # Discounts rewards in reverse\n",
    "    for i in reversed(range(len(rewards))):\n",
    "\n",
    "        # Discount fowards from the future for previous states\n",
    "        discount = rewards[i] + discount*GAMMA \n",
    "        discounted_rewards[i] = discount\n",
    "\n",
    "    return to_torch(states).mean(dim=3).unsqueeze(dim=1), to_torch(actions), to_torch(discounted_rewards).reshape(-1,1), to_torch(log_actions), timesteps, np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_EPSILON = 0.2\n",
    "value_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "def get_log_probs_and_value(states, old_actions):\n",
    "    actions, values = agent(states)\n",
    "    distribution = MultivariateNormal(actions, cov_mat)\n",
    "    log_actions = distribution.log_prob(old_actions)\n",
    "\n",
    "    return actions, log_actions, values, distribution.entropy()\n",
    "\n",
    "def compute_advantages(states, rewards):\n",
    "    _, values = agent(states)\n",
    "\n",
    "    advantages = rewards - values.detach()\n",
    "    advantages = (advantages - advantages.mean()) / \\\n",
    "                (advantages.std() + 1e-8) \n",
    "\n",
    "    return advantages\n",
    "\n",
    "def compute_losses(states, actions, rewards, log_actions, advantages):\n",
    "    # Compute policy loss first\n",
    "    # Compute ratios \n",
    "    new_actions, log_new_actions, values, entropy = get_log_probs_and_value(states, actions)\n",
    "    ratios = torch.exp(log_new_actions - log_actions) \n",
    "\n",
    "    policy_loss = torch.min(ratios*advantages, torch.clip(ratios, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON)*advantages)\n",
    "    policy_loss = -torch.mean(policy_loss)\n",
    "\n",
    "    # Compute value loss\n",
    "    value_loss = value_loss_fn(rewards, values)\n",
    "\n",
    "    # Compute entropy loss\n",
    "    entropy_loss = -torch.mean(entropy)\n",
    "    \n",
    "    return policy_loss, value_loss, entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOS\n",
    "# 1. new_actions / actions - > en log_probs (log_new_actions - log_actions)\n",
    "# 2. Trouver des meilleurs manières pour explorer\n",
    "# 3. tweak value_factor, GAMMA, exploration_factor, lr \n",
    "# 4. Tweak le modèle pytorch, activations\n",
    "\n",
    "\n",
    "def train():\n",
    "    n_time_steps = 1000000\n",
    "    n_updates_per_episode = 5\n",
    "    \n",
    "    value_factor = 0.3\n",
    "    entropy_factor = 0.01\n",
    "\n",
    "    current_time_step = 0\n",
    "\n",
    "    agent_optimizer = optim.Adam(agent.parameters(), lr=0.0001)\n",
    "    scores = []\n",
    "    while current_time_step < n_time_steps:\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                agent.eval()\n",
    "                states, actions, rewards, log_actions, timesteps, episode_score = rollout(len(scores)%5==0)\n",
    "        except:\n",
    "            continue\n",
    "        scores.append(episode_score)\n",
    "        print(f\"Current score: {scores[-1]}\")\n",
    "        print(f\"Total timesteps: {current_time_step}\")\n",
    "        current_time_step += timesteps\n",
    "        print(cov_exploration_matrix)\n",
    "        advantages = compute_advantages(states, rewards)\n",
    "\n",
    "        agent.train()\n",
    "        for _ in range(n_updates_per_episode):\n",
    "            agent_optimizer.zero_grad()\n",
    "            policy_loss, value_loss, entropy_loss = compute_losses(states, actions, rewards, log_actions, advantages)\n",
    "\n",
    "            loss = policy_loss + value_factor*value_loss #+ entropy_factor*entropy_loss\n",
    "            print(f'loss: {loss}')\n",
    "            loss.backward()\n",
    "\n",
    "            agent_optimizer.step()\n",
    "            agent_optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'eval'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-273f4207579e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd08420e9e93c695d97be91f5272eab9214f2b8a154faa67cd6133f765f4ca85a58",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "8420e9e93c695d97be91f5272eab9214f2b8a154faa67cd6133f765f4ca85a58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from scipy.stats import multivariate_normal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(CNN_Network, self).__init__()\n",
    "\n",
    "        self.conv2d_0 = nn.Conv2d(1, 8, kernel_size=4, stride=2)\n",
    "        self.relu_0 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(8, 16, kernel_size=3, stride=2)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_3 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.relu_3 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu_4 = nn.ReLU()\n",
    "\n",
    "        self.conv2d_5 = nn.Conv2d(128, 256, kernel_size=3, stride=2)\n",
    "        self.relu_5 = nn.ReLU()\n",
    "\n",
    "        self.action_linear_1 = nn.Linear(256, 128)\n",
    "        self.relu_action_1 = nn.ReLU()\n",
    "        self.action_linear_2 = nn.Linear(128, output_size)\n",
    "\n",
    "        self.value_linear_1 = nn.Linear(256, 128)\n",
    "        self.relu_value_1 = nn.ReLU()\n",
    "        self.value_linear_2 = nn.Linear(128, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu_0(self.conv2d_0(x))\n",
    "        x = self.relu_1(self.conv2d_1(x))\n",
    "        x = self.relu_2(self.conv2d_2(x))\n",
    "        x = self.relu_3(self.conv2d_3(x))\n",
    "        x = self.relu_4(self.conv2d_4(x))\n",
    "        x = self.relu_5(self.conv2d_5(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x_action = self.relu_action_1(self.action_linear_1(x))\n",
    "        x_action = self.action_linear_2(x_action)\n",
    "\n",
    "        x_value = self.relu_value_1(self.value_linear_1(x))\n",
    "        x_value = self.value_linear_2(x_value)\n",
    "        return x_action, x_value\n",
    "\n",
    "agent = CNN_Network(env.action_space.shape[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CNN_Network(\n  (conv2d_0): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2))\n  (relu_0): ReLU()\n  (conv2d_1): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n  (relu_1): ReLU()\n  (conv2d_2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n  (relu_2): ReLU()\n  (conv2d_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n  (relu_3): ReLU()\n  (conv2d_4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (relu_4): ReLU()\n  (conv2d_5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n  (relu_5): ReLU()\n  (action_linear_1): Linear(in_features=256, out_features=128, bias=True)\n  (relu_action_1): ReLU()\n  (action_linear_2): Linear(in_features=128, out_features=3, bias=True)\n  (value_linear_1): Linear(in_features=256, out_features=128, bias=True)\n  (relu_value_1): ReLU()\n  (value_linear_2): Linear(in_features=128, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class OrnsteinUhlenbeckActionNoise():\n",
    "#     def __init__(self, mu, sigma=0.3, theta=.10, dt=1e-2, x0=None):\n",
    "#         self.theta = theta\n",
    "#         self.mu = mu\n",
    "#         self.sigma = sigma\n",
    "#         self.dt = dt\n",
    "#         self.x0 = x0\n",
    "#         self.reset()\n",
    "\n",
    "#     def __call__(self):\n",
    "#         x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "#                 self.sigma * self.dt**(1/2) * torch.normal(mean=0.0, std=1.0, size=self.mu.shape, device=device)\n",
    "#         self.x_prev = x\n",
    "#         return x\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.x_prev = self.x0 if self.x0 is not None else torch.zeros_like(self.mu, device=device)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(tensor):\n",
    "    return torch.tensor(tensor.copy(), dtype=torch.float32, device=device)\n",
    "\n",
    "cov_exploration_matrix = torch.diag(torch.full((3,), 0.2)).to(device)\n",
    "def get_action(state):\n",
    "    actions, values = agent(state)\n",
    "    actions = actions[0]\n",
    "    distribution = MultivariateNormal(actions, cov_exploration_matrix)\n",
    "    log_actions = distribution.log_prob(actions)\n",
    "\n",
    "    return actions.cpu().numpy(), log_actions.cpu().numpy()\n",
    "\n",
    "GAMMA = 0.975\n",
    "def rollout(noise_factor, render):\n",
    "\n",
    "    state = env.reset()\n",
    "    memory = []\n",
    "    timesteps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        state = state / 255.\n",
    "        action, log_action = get_action(to_torch(state).mean(dim=2).reshape(1, 1, state.shape[0], state.shape[1]))\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.append([state, action, reward, log_action])\n",
    "        timesteps += 1\n",
    "        state = next_state\n",
    "\n",
    "    states, actions, rewards, log_actions = map(np.array, zip(*memory))\n",
    "    plt.show()\n",
    "\n",
    "    discounted_rewards = np.zeros((len(rewards)))\n",
    "    discount = 0\n",
    "    # Discounts rewards in reverse\n",
    "    for i in reversed(range(len(rewards))):\n",
    "\n",
    "        # Discount fowards from the future for previous states\n",
    "        discount = rewards[i] + discount*GAMMA \n",
    "        discounted_rewards[i] = discount\n",
    "\n",
    "    return to_torch(states).mean(dim=3).unsqueeze(dim=1), to_torch(actions), to_torch(discounted_rewards), to_torch(log_actions), timesteps, np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_EPSILON = 0.2\n",
    "value_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "def get_log_probs_and_value(states):\n",
    "    actions, values = agent(states)\n",
    "    distribution = MultivariateNormal(actions, cov_exploration_matrix)\n",
    "    log_actions = distribution.log_prob(actions)\n",
    "\n",
    "\n",
    "    return log_actions, values\n",
    "\n",
    "def compute_losses(states, actions, rewards, log_actions):\n",
    "    # Compute policy loss first\n",
    "    # Compute ratios \n",
    "    log_new_actions, values = get_log_probs_and_value(states)\n",
    "    ratios = torch.exp(log_new_actions - log_actions)\n",
    "\n",
    "    advantages = rewards.unsqueeze(dim=1) - values.detach()\n",
    "\n",
    "    policy_loss = torch.min(ratios*advantages, torch.clip(ratios, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON)*advantages)\n",
    "    policy_loss = -torch.mean(policy_loss)\n",
    "\n",
    "    # Compute value loss\n",
    "    value_loss = value_loss_fn(rewards, values)\n",
    "\n",
    "    # Compute entropy loss\n",
    "    # TODO\n",
    "    \n",
    "    return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOS\n",
    "# 1. new_actions / actions - > en log_probs (log_new_actions - log_actions)\n",
    "# 2. Trouver des meilleurs manières pour explorer\n",
    "# 3. tweak value_factor, GAMMA, exploration_factor, lr \n",
    "# 4. Tweak le modèle pytorch, activations\n",
    "\n",
    "\n",
    "def train():\n",
    "    n_time_steps = 1000000\n",
    "    n_updates_per_episode = 2\n",
    "    \n",
    "    value_factor = 0.3\n",
    "\n",
    "    current_time_step = 0\n",
    "    exploration_factor = 0.1\n",
    "    exploration_decay = 0.995\n",
    "\n",
    "    agent_optimizer = optim.Adam(agent.parameters(), lr=0.0003)\n",
    "    scores = []\n",
    "    while current_time_step < n_time_steps:\n",
    "        # try:\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, log_actions, timesteps, episode_score = rollout(exploration_factor*exploration_decay**len(scores), len(scores)%20==0)\n",
    "        # except:\n",
    "        #     continue\n",
    "        scores.append(episode_score)\n",
    "        print(f\"Current score: {scores[-1]}\")\n",
    "        print(f\"Total timesteps: {current_time_step}\")\n",
    "        current_time_step += timesteps\n",
    "\n",
    "        for _ in range(n_updates_per_episode):\n",
    "\n",
    "            agent_optimizer.zero_grad()\n",
    "            policy_loss, value_loss = compute_losses(states, actions, rewards, log_actions)\n",
    "\n",
    "            loss = policy_loss + value_factor*value_loss #- 10 * torch.mean(torch.std(actions, dim=0))\n",
    "            print(f'loss: {loss}')\n",
    "            loss.backward()\n",
    "\n",
    "            agent_optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "Current score: -72.02797202797214\n",
      "Total timesteps: 0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-8b1e1317a6d1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0magent_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mpolicy_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue_factor\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalue_loss\u001b[0m \u001b[1;31m#- 10 * torch.mean(torch.std(actions, dim=0))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-629022fa16b1>\u001b[0m in \u001b[0;36mcompute_losses\u001b[1;34m(states, actions, rewards, log_actions)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Compute policy loss first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Compute ratios\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mlog_new_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_log_probs_and_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mratios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_new_actions\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlog_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-629022fa16b1>\u001b[0m in \u001b[0;36mget_log_probs_and_value\u001b[1;34m(states)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_log_probs_and_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_exploration_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlog_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}